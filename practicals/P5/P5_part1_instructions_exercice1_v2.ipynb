{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c235c577-43a5-4698-9c3e-f92245eeb63a",
      "metadata": {
        "id": "c235c577-43a5-4698-9c3e-f92245eeb63a"
      },
      "source": [
        "# Practical session n°5\n",
        "\n",
        "Notions:\n",
        "- Semantic segmentation\n",
        "- Image Denoising\n",
        "- Fully convolutional networks, U-Net\n",
        "- Weak supervision (in part II): The noise-to-noise and the Neural Eggs Separation scenarios.\n",
        "\n",
        "Duration: 1 h 30 + 2 h\n",
        "\n",
        "In P2, we illustrated how Convolutional Neural Networks (CNNs) are trained for image classification tasks. In this practical session, we demonstrate how to achieve pixel-level predictions for tasks like semantic segmentation and image denoising.\n",
        "\n",
        "To start, we’ll simply apply an off-the-shelf model. Then, we’ll focus on training a model from scratch (part I, exercise 2 and part II).\n",
        "\n",
        "In P3, we also introduced a crucial set of methods known as \"transfer learning,\" which is particularly effective when there’s limited training data. In this session, we’ll explore another equally important set of methods called \"weak supervision,\" which is well-suited for cases where ground truth is imperfectly known (Part II).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e922d59-300f-4e0d-ad71-3f584bb87089",
      "metadata": {
        "id": "4e922d59-300f-4e0d-ad71-3f584bb87089"
      },
      "source": [
        "## Part I: Semantic Segmentation and Image Denoising with Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322901ab-9073-4620-be90-f264fa14ec9c",
      "metadata": {
        "id": "322901ab-9073-4620-be90-f264fa14ec9c"
      },
      "source": [
        "This part aims to familiarize you with a semantic segmentation task.\n",
        "\n",
        "By definition, a Fully Convolutional Network (FCN) does not contain fully connected layers. As a result, the output retains spatial dimensions. This configuration is useful when the learning target itself is an image. This is the case for tasks such as:\n",
        "- Semantic segmentation, where each pixel is assigned a semantic class (e.g., ground, sky, clouds, buildings, etc.).\n",
        "- Pixel-wise regression\n",
        "- Image denoising\n",
        "- Super-resolution\n",
        "\n",
        "The first exercise features an FCN built from a ResNet50 for a simple segmentation task defined from a set of real images segmented by hand.\n",
        "\n",
        "The second exercise proposes a pixel-wise regression task, completely supervised, defined on a set of dynamically generated synthetic images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Semantic Segmentation with FCN-ResNet (no training)**"
      ],
      "metadata": {
        "id": "4VEEDrnUIB-C"
      },
      "id": "4VEEDrnUIB-C"
    },
    {
      "cell_type": "markdown",
      "id": "b7a950ac-4a53-419e-ba51-32c2961ed8b5",
      "metadata": {
        "id": "b7a950ac-4a53-419e-ba51-32c2961ed8b5"
      },
      "source": [
        "**A.** Presentation of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c946232-77c7-48a6-97a3-b859dc1d4e98",
      "metadata": {
        "id": "3c946232-77c7-48a6-97a3-b859dc1d4e98"
      },
      "source": [
        "In the following cells, we load the necessary libraries, download the set of segmented images prepared for the [Pascal VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/segexamples/index.html) challenge, and visualize input-target pairs from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d9a87a8a-065c-4d96-9056-96fe3f2f6d89",
      "metadata": {
        "id": "d9a87a8a-065c-4d96-9056-96fe3f2f6d89"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cba2f280-3609-4fc6-9d47-ba9eb29236e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cba2f280-3609-4fc6-9d47-ba9eb29236e7",
        "outputId": "d2e91572-1fb4-49de-aa59-5e639151aa09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are on GPU !\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"You are on GPU !\")\n",
        "else:\n",
        "  print('Change the runtime to GPU or continue with CPU, but this should slow down your trainings')\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root = \"/content/data\"\n",
        "os.makedirs(root, exist_ok=True)\n",
        "\n",
        "# Download from BrainChip mirror\n",
        "!wget http://data.brainchip.com/dataset-mirror/voc/VOCtrainval_06-Nov-2007.tar \\\n",
        "     -O /content/data/VOCtrainval_06-Nov-2007.tar\n",
        "! tar -xf /content/data/VOCtrainval_06-Nov-2007.tar -C /content/data"
      ],
      "metadata": {
        "id": "fVra1X946l3L",
        "outputId": "47c87df1-27b0-4093-9a6b-48f9013b50fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "fVra1X946l3L",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-28 11:06:02--  http://data.brainchip.com/dataset-mirror/voc/VOCtrainval_06-Nov-2007.tar\n",
            "Resolving data.brainchip.com (data.brainchip.com)... 146.59.209.152, 2001:41d0:301::31\n",
            "Connecting to data.brainchip.com (data.brainchip.com)|146.59.209.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://data.brainchip.com/dataset-mirror/voc/VOCtrainval_06-Nov-2007.tar [following]\n",
            "--2025-11-28 11:06:02--  https://data.brainchip.com/dataset-mirror/voc/VOCtrainval_06-Nov-2007.tar\n",
            "Connecting to data.brainchip.com (data.brainchip.com)|146.59.209.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460032000 (439M) [application/x-tar]\n",
            "Saving to: ‘/content/data/VOCtrainval_06-Nov-2007.tar’\n",
            "\n",
            "/content/data/VOCtr 100%[===================>] 438.72M  70.4MB/s    in 6.5s    \n",
            "\n",
            "2025-11-28 11:06:09 (67.2 MB/s) - ‘/content/data/VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check-up\n",
        "\n",
        "! ls /content/data\n",
        "# should show: VOCdevkit  VOCtrainval_06-Nov-2007.tar\n",
        "\n",
        "! ls /content/data/VOCdevkit\n",
        "# should show: VOC2007\n",
        "\n",
        "! ls /content/data/VOCdevkit/VOC2007\n",
        "# should show: JPEGImages  SegmentationClass  SegmentationObject  ImageSets  ...\n"
      ],
      "metadata": {
        "id": "4UwF09mDHODH",
        "outputId": "762aec90-24f4-4761-a07f-d20fb50403b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4UwF09mDHODH",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCdevkit  VOCtrainval_06-Nov-2007.tar\n",
            "VOC2007\n",
            "Annotations  ImageSets\tJPEGImages  SegmentationClass  SegmentationObject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3bb6b382-a270-4e4e-b7e5-95943b600159",
      "metadata": {
        "id": "3bb6b382-a270-4e4e-b7e5-95943b600159"
      },
      "outputs": [],
      "source": [
        "# Build the training dataset\n",
        "root = \"/content/data\"\n",
        "input_resize = transforms.Resize((128, 128))\n",
        "target_resize = transforms.Resize((128, 128))\n",
        "\n",
        "train_dataset_viz = datasets.VOCSegmentation(\n",
        "    root,\n",
        "    year='2007',\n",
        "    image_set='train',\n",
        "    download = False,\n",
        "    transform=input_resize,\n",
        "    target_transform=target_resize,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8f19ad-c3db-4b4a-a3f4-d7b90941b0bc",
      "metadata": {
        "id": "bd8f19ad-c3db-4b4a-a3f4-d7b90941b0bc"
      },
      "outputs": [],
      "source": [
        "# Viz some images\n",
        "\n",
        "import math\n",
        "\n",
        "def plot_images(images, num_per_row=4, title=None):\n",
        "    num_rows = int(math.ceil(len(images) / num_per_row))\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_per_row,figsize=(4*num_per_row,4*num_rows))\n",
        "    #fig.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "    for image, ax in zip(images, axes.flat):\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Sampling for viz:\n",
        "\n",
        "inputs, ground_truths = list(zip(*[train_dataset_viz[i] for i in range(8)]))\n",
        "\n",
        "_ = plot_images(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74e9396-8ed9-4729-9142-4fd1a6241366",
      "metadata": {
        "id": "e74e9396-8ed9-4729-9142-4fd1a6241366"
      },
      "outputs": [],
      "source": [
        "# Viz some targets\n",
        "\n",
        "_ = plot_images(ground_truths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0478d313-a90d-420a-a74a-2e6c78b5ae44",
      "metadata": {
        "id": "0478d313-a90d-420a-a74a-2e6c78b5ae44"
      },
      "source": [
        "**Question 1:** How many classes are there? \\\n",
        "Search the web for the difference between semantic segmentation and instance segmentation. What type of segmentation is this dataset about?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TSQMGJaS5Pqs",
      "metadata": {
        "id": "TSQMGJaS5Pqs",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9636b6792aadc531",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# num_classes = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MC1WQOva5s8j",
      "metadata": {
        "id": "MC1WQOva5s8j",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b2ebe011b0b5e6f6",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "\n",
        "1. **Semantic Segmentation:**\n",
        "   - **Objective:** Assign a class label to each pixel in an image, indicating the category or type of the object to which it belongs.\n",
        "   - **Target:** The segmentation masks indicate the class or category of each pixel.\n",
        "\n",
        "\n",
        "2. **Instance Segmentation:**\n",
        "   - **Objective:** Identify and outline individual objects in an image.\n",
        "   - **Target:** The mask contains the same value for each set of pixels associated with the same physical object.\n",
        "\n",
        "\n",
        "\n",
        "Here, the segmentation task is a semantic segmentation, where each pixel is assigned a class label representing the category of the object to which it belongs. The classes may include categories like ground, sky, clouds, buildings, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad29c10-d73a-4e9e-a68f-3de640091792",
      "metadata": {
        "id": "2ad29c10-d73a-4e9e-a68f-3de640091792"
      },
      "source": [
        "**B.** Presentation of an FCN-ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67298d17-176b-4f62-af9b-083aee1b52c3",
      "metadata": {
        "id": "67298d17-176b-4f62-af9b-083aee1b52c3"
      },
      "source": [
        "In the next cell, we load an [FCN](https://pytorch.org/vision/stable/models/fcn.html) built from a [ResNet50](https://arxiv.org/pdf/1512.03385.pdf). This model achieved state-of-the-art performance in 2015 according to [paperswithcode](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30500a47-f15e-420a-83eb-1f977a4fbedd",
      "metadata": {
        "id": "30500a47-f15e-420a-83eb-1f977a4fbedd"
      },
      "outputs": [],
      "source": [
        "fcn = torchvision.models.segmentation.fcn_resnet50(weights_backbone = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13c65d5-8231-4071-99ec-a1295f2bd924",
      "metadata": {
        "id": "c13c65d5-8231-4071-99ec-a1295f2bd924"
      },
      "source": [
        "**Q2:** What is different from a standard ResNet50? Does the FCN provide an output of the same size as the input? Test and explain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample the dataset, convert to torch.tensor:\n",
        "batch_size = 4\n",
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "input_resize = transforms.Resize([64,64])\n",
        "target_resize = transforms.Resize([64,64])\n",
        "\n",
        "# Transforms used during the training :\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def replace_tensor_value_(tensor, a, b):\n",
        "    tensor[tensor == a] = b\n",
        "    return tensor\n",
        "\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 21)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Def of the Dataset object :\n",
        "train_dataset = datasets.VOCSegmentation(\n",
        "    './data',\n",
        "    year='2007',\n",
        "    download=False,\n",
        "    image_set='train',\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "inputs, targets  = next(iter(train_loader))\n",
        "\n",
        "\n",
        "# Test here :\n",
        "..."
      ],
      "metadata": {
        "id": "5BPa-_fhNg6q"
      },
      "id": "5BPa-_fhNg6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f354b3a6-fecb-4fe3-8b9e-ad3ff5f62b67",
      "metadata": {
        "id": "f354b3a6-fecb-4fe3-8b9e-ad3ff5f62b67"
      },
      "source": [
        "**C. Testing a Pretrained Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5be37f2-bf35-4060-97a8-0f3d8c0c698a",
      "metadata": {
        "id": "c5be37f2-bf35-4060-97a8-0f3d8c0c698a"
      },
      "source": [
        "In this exercise, we simply **test** a model trained on another segmentation dataset. An extension to this exercise provides an opportunity to train the model introduced in exercise sheet #2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad4a0f3-108f-47f5-b7fe-7452547a4c27",
      "metadata": {
        "id": "8ad4a0f3-108f-47f5-b7fe-7452547a4c27"
      },
      "outputs": [],
      "source": [
        "fcn = torchvision.models.segmentation.fcn_resnet50(weights='COCO_WITH_VOC_LABELS_V1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ab4d24-23d5-487c-9dbe-e5625fbfc44d",
      "metadata": {
        "id": "31ab4d24-23d5-487c-9dbe-e5625fbfc44d"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "input_resize = transforms.Resize((256,256))\n",
        "target_resize = transforms.Resize((256,256))\n",
        "\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_dataset = datasets.VOCSegmentation(\n",
        "    './data',\n",
        "    year='2007',\n",
        "    download=False,\n",
        "    image_set='val',\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c498bcc-fcfc-44f2-a30f-2a0747e04075",
      "metadata": {
        "id": "6c498bcc-fcfc-44f2-a30f-2a0747e04075"
      },
      "outputs": [],
      "source": [
        "# Creating loaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6l5HcC7VAq4C",
      "metadata": {
        "id": "6l5HcC7VAq4C",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-840366ac0521f360",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**Q3:** According to the preceding code lines, which set (validation or test) of PascalVOC2007 are we testing the model on? Why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c815484-4636-4659-90dc-52af6f299b69",
      "metadata": {
        "id": "2c815484-4636-4659-90dc-52af6f299b69"
      },
      "source": [
        "Now, let's visualize some model outputs on this set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e20b2e-6317-4c3c-b099-cf4fef58b5fe",
      "metadata": {
        "id": "73e20b2e-6317-4c3c-b099-cf4fef58b5fe"
      },
      "outputs": [],
      "source": [
        "# Color palette for segmentation masks\n",
        "PALETTE = np.array(\n",
        "    [\n",
        "        [0, 0, 0],\n",
        "        [128, 0, 0],\n",
        "        [0, 128, 0],\n",
        "        [128, 128, 0],\n",
        "        [0, 0, 128],\n",
        "        [128, 0, 128],\n",
        "        [0, 128, 128],\n",
        "        [128, 128, 128],\n",
        "        [64, 0, 0],\n",
        "        [192, 0, 0],\n",
        "        [64, 128, 0],\n",
        "        [192, 128, 0],\n",
        "        [64, 0, 128],\n",
        "        [192, 0, 128],\n",
        "        [64, 128, 128],\n",
        "        [192, 128, 128],\n",
        "        [0, 64, 0],\n",
        "        [128, 64, 0],\n",
        "        [0, 192, 0],\n",
        "        [128, 192, 0],\n",
        "        [0, 64, 128],\n",
        "    ]\n",
        "    + [[0, 0, 0] for i in range(256 - 22)]\n",
        "    + [[255, 255, 255]],\n",
        "    dtype=np.uint8,\n",
        ")\n",
        "\n",
        "\n",
        "def array1d_to_pil_image(array):\n",
        "    pil_out = Image.fromarray(array.astype(np.uint8), mode='P')\n",
        "    pil_out.putpalette(PALETTE)\n",
        "    return pil_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bfa5b3-75c9-4af1-9b7c-f0cfbd02355d",
      "metadata": {
        "id": "d0bfa5b3-75c9-4af1-9b7c-f0cfbd02355d"
      },
      "outputs": [],
      "source": [
        "inputs, targets = next(iter(test_loader))\n",
        "outputs = fcn(inputs)['out']\n",
        "outputs = outputs.argmax(1)\n",
        "\n",
        "outputs = replace_tensor_value_(outputs, 21, 255)\n",
        "targets = replace_tensor_value_(targets, 21, 255)\n",
        "targets = targets.squeeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7e48ab-584f-4c58-8284-881c29b6fa37",
      "metadata": {
        "id": "9c7e48ab-584f-4c58-8284-881c29b6fa37"
      },
      "outputs": [],
      "source": [
        "plt_inputs = np.clip(inputs.numpy().transpose((0, 2, 3, 1)) * imagenet_std + imagenet_mean, 0, 1)\n",
        "fig = plot_images(plt_inputs)\n",
        "fig.suptitle(\"Images\")\n",
        "\n",
        "pil_outputs = [array1d_to_pil_image(out) for out in outputs.numpy()]\n",
        "fig = plot_images(pil_outputs)\n",
        "fig.suptitle(\"Predictions\")\n",
        "\n",
        "pil_targets = [array1d_to_pil_image(gt) for gt in targets.numpy()]\n",
        "fig = plot_images(pil_targets)\n",
        "_ = fig.suptitle(\"Ground truths\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ffe212-798a-4ee7-9f9f-f3ce99283f66",
      "metadata": {
        "id": "d8ffe212-798a-4ee7-9f9f-f3ce99283f66"
      },
      "source": [
        "Finally, let's evaluate the model on the entire set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92deea8-818e-4dfb-bb61-ce952f86188b",
      "metadata": {
        "id": "f92deea8-818e-4dfb-bb61-ce952f86188b"
      },
      "outputs": [],
      "source": [
        "# For the test metric\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cc4832-79eb-403e-a665-ecb8371b75e2",
      "metadata": {
        "id": "c1cc4832-79eb-403e-a665-ecb8371b75e2"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "IoU = torchmetrics.JaccardIndex(num_classes=21, ignore_index=255,task=\"multiclass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bffe4f6-c159-4e3d-9200-47c2afa85d9f",
      "metadata": {
        "id": "4bffe4f6-c159-4e3d-9200-47c2afa85d9f",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d0d00b1251df1c27",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "Q4: Jaccard Index is used instead of accuracy. How is it defined? What is its other name? What is its advantage?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ccfd8be-aeda-4d7a-8f3a-7c97103f4caf",
      "metadata": {
        "id": "1ccfd8be-aeda-4d7a-8f3a-7c97103f4caf"
      },
      "source": [
        "Q5: Modify the following code to obtain an average IoU over the entire set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e346cbb-87b2-4481-9856-f12cd989f20f",
      "metadata": {
        "id": "0e346cbb-87b2-4481-9856-f12cd989f20f"
      },
      "outputs": [],
      "source": [
        "fcn = fcn.to(device)\n",
        "fcn.eval()\n",
        "nbatch = 0\n",
        "sum_batch_IoU = 0\n",
        "for i, (inputs, targets) in enumerate(test_loader):\n",
        "  ...\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}