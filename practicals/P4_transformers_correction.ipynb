{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db6482b",
   "metadata": {},
   "source": [
    "\n",
    "# P4_transformers_correction\n",
    "\n",
    "**Practical session: Transformers vs CNNs (Dyck validation, Addition, Parity)**  \n",
    "Colab-ready, modular forward pass, timing & FLOPs, and controlled CNN baselines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8d855",
   "metadata": {},
   "source": [
    "\n",
    "### **I.A.** Dyck-1 (balanced parentheses) dataset & validation target\n",
    "\n",
    "Goal: binary classification (valid vs invalid) of sequences over the alphabet `{ '(', ')' }`, with optional distractor tokens.\n",
    "- We'll generate balanced (valid) and corrupted (invalid) sequences of variable length.\n",
    "- Baselines: naive 1D CNN vs dilated CNN vs small Transformer.\n",
    "- Expectation: naive CNN struggles on long-range nesting; dilation helps; Transformer handles it cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Imports & utilities\n",
    "import math, random, time, os, sys\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Simple seed control\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); \n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(0)\n",
    "\n",
    "# Vocab and helpers\n",
    "VOCAB = ['(', ')', 'x']  # 'x' optional distractor\n",
    "stoi = {c:i for i,c in enumerate(VOCAB)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "\n",
    "def gen_dyck1_seq(n_pairs, p_distractor=0.0, max_distractors=0):\n",
    "    \"\"\"Generate a (possibly) valid Dyck-1 sequence (string) and a label (1 valid / 0 invalid).\"\"\"\n",
    "    # build a valid sequence by sampling a random Dyck path via push/pop\n",
    "    seq = []\n",
    "    depth = 0\n",
    "    for _ in range(2*n_pairs):\n",
    "        # ensure validity while allowing some randomness\n",
    "        if depth == 0:\n",
    "            seq.append('('); depth += 1\n",
    "        elif depth == (2*n_pairs - len(seq)):\n",
    "            seq.append(')'); depth -= 1\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                seq.append('('); depth += 1\n",
    "            else:\n",
    "                seq.append(')'); depth -= 1\n",
    "    assert depth == 0\n",
    "\n",
    "    # with 50% probability, corrupt to make an invalid example\n",
    "    is_valid = (random.random() < 0.5)\n",
    "    if not is_valid:\n",
    "        # minimal corruption: flip a random token if possible\n",
    "        pos = random.randrange(len(seq))\n",
    "        seq[pos] = '(' if seq[pos] == ')' else ')'\n",
    "        # ensure it is actually invalid (fall back to another corruption if needed)\n",
    "        # quick validator\n",
    "        if validate_dyck1(''.join(seq)):\n",
    "            # do another flip\n",
    "            pos2 = (pos + 1) % len(seq)\n",
    "            seq[pos2] = '(' if seq[pos2] == ')' else ')'\n",
    "\n",
    "    # inject distractors\n",
    "    if p_distractor > 0 and max_distractors > 0:\n",
    "        k = random.randint(0, max_distractors)\n",
    "        for _ in range(k):\n",
    "            if random.random() < p_distractor:\n",
    "                j = random.randrange(len(seq)+1)\n",
    "                seq.insert(j, 'x')\n",
    "\n",
    "    return ''.join(seq), int(is_valid)\n",
    "\n",
    "def validate_dyck1(s):\n",
    "    depth = 0\n",
    "    for ch in s:\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "        elif ch == ')':\n",
    "            depth -= 1\n",
    "            if depth < 0: return False\n",
    "        else:\n",
    "            # ignore distractors\n",
    "            pass\n",
    "    return depth == 0\n",
    "\n",
    "class DyckDataset(Dataset):\n",
    "    def __init__(self, n_samples=20000, n_pairs_range=(2, 40), p_distractor=0.0, max_distractors=0):\n",
    "        self.samples = []\n",
    "        for _ in range(n_samples):\n",
    "            n_pairs = random.randint(*n_pairs_range)\n",
    "            s, y = gen_dyck1_seq(n_pairs, p_distractor, max_distractors)\n",
    "            x = torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "            self.samples.append((x, torch.tensor(y, dtype=torch.long)))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "def pad_collate(batch, pad_idx=0):\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "    maxlen = int(lengths.max())\n",
    "    X = torch.full((len(xs), maxlen), pad_idx, dtype=torch.long)\n",
    "    for i, x in enumerate(xs):\n",
    "        X[i, :len(x)] = x\n",
    "    Y = torch.stack(ys)\n",
    "    return X, lengths, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125fabc",
   "metadata": {},
   "source": [
    "\n",
    "### **I.B.** Naive 1D CNN baseline (no dilation)\n",
    "\n",
    "A small temporal CNN with limited receptive field; we pool over time for the final binary decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8767f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NaiveCNN(nn.Module):\n",
    "    def __init__(self, vocab_size=len(VOCAB), emb_dim=32, hidden=64, n_layers=2, kernel=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        layers = []\n",
    "        c_in = emb_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers += [nn.Conv1d(c_in, hidden, kernel_size=kernel, padding=kernel//2),\n",
    "                       nn.ReLU()]\n",
    "            c_in = hidden\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x: (B, L)\n",
    "        e = self.emb(x).transpose(1, 2)   # (B, C, L)\n",
    "        h = self.net(e)                   # (B, H, L)\n",
    "        h = h.mean(dim=-1)                # global average pool over time\n",
    "        return self.head(h)               # (B, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcba45a",
   "metadata": {},
   "source": [
    "\n",
    "### **I.C.** Training loop (shared)\n",
    "\n",
    "We'll reuse the same training/eval utilities for all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 128\n",
    "    lr: float = 2e-3\n",
    "    max_batches_per_epoch: int = 200  # cap for speed in class\n",
    "    clip_grad: float = 1.0\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=-1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(model, loader, opt=None):\n",
    "    is_train = opt is not None\n",
    "    model.train(is_train)\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for i, (X, lengths, Y) in enumerate(loader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits = model(X, lengths)\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "        if is_train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            if cfg.clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad)\n",
    "            opt.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        total_acc  += accuracy(logits, Y) * X.size(0)\n",
    "        n += X.size(0)\n",
    "        if is_train and cfg.max_batches_per_epoch and i+1 >= cfg.max_batches_per_epoch:\n",
    "            break\n",
    "    return total_loss/n, total_acc/n\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, cfg: TrainConfig):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "    best = {\"val_acc\": 0.0, \"state\": None}\n",
    "    for ep in range(cfg.epochs):\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, opt)\n",
    "        va_loss, va_acc = run_epoch(model, val_loader, None)\n",
    "        if va_acc > best[\"val_acc\"]:\n",
    "            best = {\"val_acc\": va_acc, \"state\": {k:v.detach().cpu() for k,v in model.state_dict().items()}}\n",
    "        print(f\"Epoch {ep+1}/{cfg.epochs} | train acc {tr_acc:.3f} | val acc {va_acc:.3f}\")\n",
    "    # load best\n",
    "    model.load_state_dict(best[\"state\"])\n",
    "    return model, best[\"val_acc\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6511e497",
   "metadata": {},
   "source": [
    "\n",
    "### **II.A.** Minimal Transformer (modular forward pass)\n",
    "\n",
    "We decompose the forward pass to mirror the step-by-step style used in *BE_session2_exercice*:\n",
    "- `token_embed` + `pos_embed`\n",
    "- Linear projections to Q, K, V\n",
    "- Scaled dot-product attention\n",
    "- Multi-head split/merge\n",
    "- MLP (feed-forward) per token\n",
    "- Residual + LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20740ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=4096):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, L, D)\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:, :L, :]\n",
    "\n",
    "def attention_scores(Q, K, mask=None):\n",
    "    # Q,K: (B, H, L, Dh)\n",
    "    Dh = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Dh)  # (B,H,L,L)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    P = torch.softmax(scores, dim=-1)\n",
    "    return P\n",
    "\n",
    "def attention_apply(P, V):\n",
    "    # P: (B,H,L,L), V:(B,H,L,Dh)\n",
    "    return torch.matmul(P, V)  # (B,H,L,Dh)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=128, n_heads=4, dropout=0.0, causal=False):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_model // n_heads\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.causal = causal\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        q = self.Wq(x).view(B, L, self.n_heads, self.d_head).transpose(1,2)  # (B,H,L,Dh)\n",
    "        k = self.Wk(x).view(B, L, self.n_heads, self.d_head).transpose(1,2)\n",
    "        v = self.Wv(x).view(B, L, self.n_heads, self.d_head).transpose(1,2)\n",
    "        # causal mask if needed\n",
    "        mask = None\n",
    "        if self.causal:\n",
    "            mask = torch.tril(torch.ones(L, L, device=x.device)).unsqueeze(0).unsqueeze(0)  # (1,1,L,L)\n",
    "        P = attention_scores(q, k, mask)        # (B,H,L,L)\n",
    "        P = self.dropout(P)\n",
    "        z = attention_apply(P, v)               # (B,H,L,Dh)\n",
    "        z = z.transpose(1,2).contiguous().view(B, L, D)  # merge heads\n",
    "        return self.Wo(z)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=128, n_heads=4, mlp_ratio=4, dropout=0.1, causal=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, n_heads, dropout=dropout, causal=causal)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model*mlp_ratio, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=len(VOCAB), d_model=128, n_layers=2, n_heads=4, num_classes=2, max_len=4096):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model, max_len=max_len)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, causal=False) for _ in range(n_layers)])\n",
    "        self.head = nn.Linear(d_model, num_classes)\n",
    "    def forward(self, x, lengths=None):\n",
    "        h = self.tok(x)\n",
    "        h = self.pos(h)\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h)\n",
    "        # classification: mean-pool over time\n",
    "        h = h.mean(dim=1)\n",
    "        return self.head(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbcc40a",
   "metadata": {},
   "source": [
    "\n",
    "### **II.B.** Addition and Parity tasks\n",
    "\n",
    "Two synthetic sequence-to-label tasks:\n",
    "- **Addition**: strings like `\"123+45=\"` → label is the sum mod 1000 (or predict each digit; here we classify the last digit for simplicity).\n",
    "- **Parity**: binary strings → label 0/1 is the parity (XOR) of ones.\n",
    "We keep them as **sequence classification** to compare architectures fairly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81937a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Addition (predict last digit of the sum) over base-10 characters\n",
    "DIGITS = [str(i) for i in range(10)]\n",
    "ADD_VOCAB = DIGITS + ['+','=']\n",
    "add_stoi = {c:i for i,c in enumerate(ADD_VOCAB)}\n",
    "add_itos = {i:c for c,i in add_stoi.items()}\n",
    "\n",
    "def gen_add_sample(n1_digits=3, n2_digits=3):\n",
    "    a = random.randint(0, 10**n1_digits - 1)\n",
    "    b = random.randint(0, 10**n2_digits - 1)\n",
    "    s = f\"{a}+{b}=\"\n",
    "    y = (a + b) % 10  # last digit\n",
    "    x = torch.tensor([add_stoi[c] for c in s], dtype=torch.long)\n",
    "    return x, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class AddDataset(Dataset):\n",
    "    def __init__(self, n, n1_digits=3, n2_digits=3):\n",
    "        self.samples = [gen_add_sample(n1_digits, n2_digits) for _ in range(n)]\n",
    "        self.num_classes = 10\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return self.samples[i]\n",
    "\n",
    "def add_pad_collate(batch, pad_idx=0):\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "    maxlen = int(lengths.max())\n",
    "    X = torch.full((len(xs), maxlen), pad_idx, dtype=torch.long)\n",
    "    for i, x in enumerate(xs):\n",
    "        X[i, :len(x)] = x\n",
    "    Y = torch.stack(ys)\n",
    "    return X, lengths, Y\n",
    "\n",
    "# Parity over binary strings\n",
    "BIN_VOCAB = ['0','1']\n",
    "bin_stoi = {c:i for i,c in enumerate(BIN_VOCAB)}\n",
    "\n",
    "def gen_parity_sample(L=64):\n",
    "    s = ''.join(random.choice(BIN_VOCAB) for _ in range(L))\n",
    "    y = s.count('1') % 2\n",
    "    x = torch.tensor([bin_stoi[c] for c in s], dtype=torch.long)\n",
    "    return x, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class ParityDataset(Dataset):\n",
    "    def __init__(self, n, L=64):\n",
    "        self.samples = [gen_parity_sample(L) for _ in range(n)]\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return self.samples[i]\n",
    "\n",
    "def bin_pad_collate(batch, pad_idx=0):\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "    maxlen = int(lengths.max())\n",
    "    X = torch.full((len(xs), maxlen), pad_idx, dtype=torch.long)\n",
    "    for i, x in enumerate(xs):\n",
    "        X[i, :len(x)] = x\n",
    "    Y = torch.stack(ys)\n",
    "    return X, lengths, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba51e1f",
   "metadata": {},
   "source": [
    "\n",
    "### **II.C.** Dilated CNN + experiment harness\n",
    "\n",
    "We compare:\n",
    "1) NaiveCNN (no dilation)  \n",
    "2) DilatedCNN (growing dilation)  \n",
    "3) TinyTransformer\n",
    "\n",
    "Same training harness; you can switch datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DilatedCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, hidden=96, layers_cfg=((3,1),(3,2),(3,4),(3,8)), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        blocks = []\n",
    "        c_in = emb_dim\n",
    "        for k, d in layers_cfg:\n",
    "            pad = (k-1)//2 * d\n",
    "            blocks += [nn.Conv1d(c_in, hidden, kernel_size=k, dilation=d, padding=pad),\n",
    "                       nn.ReLU()]\n",
    "            c_in = hidden\n",
    "        self.net = nn.Sequential(*blocks)\n",
    "        self.head = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x, lengths=None):\n",
    "        e = self.emb(x).transpose(1,2)\n",
    "        h = self.net(e)\n",
    "        h = h.mean(dim=-1)\n",
    "        return self.head(h)\n",
    "\n",
    "# Example: run a quick sanity experiment on Dyck\n",
    "cfg = TrainConfig(epochs=3, batch_size=128, lr=2e-3, max_batches_per_epoch=150)\n",
    "\n",
    "train_ds = DyckDataset(n_samples=8000, n_pairs_range=(2,30), p_distractor=0.1, max_distractors=4)\n",
    "val_ds   = DyckDataset(n_samples=2000, n_pairs_range=(2,40), p_distractor=0.1, max_distractors=6)\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=pad_collate, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=pad_collate, num_workers=0)\n",
    "\n",
    "print(\"Naive CNN on Dyck\")\n",
    "model = NaiveCNN(vocab_size=len(VOCAB), emb_dim=32, hidden=64, n_layers=2, kernel=3, num_classes=2)\n",
    "model, acc_naive = train_eval(model, train_loader, val_loader, cfg)\n",
    "\n",
    "print(\"\\nDilated CNN on Dyck\")\n",
    "model = DilatedCNN(vocab_size=len(VOCAB), emb_dim=64, hidden=96, layers_cfg=((3,1),(3,2),(3,4),(3,8)), num_classes=2)\n",
    "model, acc_dil = train_eval(model, train_loader, val_loader, cfg)\n",
    "\n",
    "print(\"\\nTransformer on Dyck\")\n",
    "model = TinyTransformer(vocab_size=len(VOCAB), d_model=128, n_layers=2, n_heads=4, num_classes=2)\n",
    "model, acc_tx = train_eval(model, train_loader, val_loader, cfg)\n",
    "\n",
    "print(f\"Val acc: naive {acc_naive:.3f} | dilated {acc_dil:.3f} | transformer {acc_tx:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56c60f",
   "metadata": {},
   "source": [
    "\n",
    "### **II.D.** FLOPs and duration vs number of attention heads\n",
    "\n",
    "We provide **(i)** a closed-form FLOPs estimate for a single Transformer block (self-attention + MLP) and **(ii)** a micro-benchmark to measure wall-clock as a function of heads `H` for fixed `d_model` and sequence length `L`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_block_flops(L, d_model, n_heads, mlp_ratio=4):\n",
    "    # QKV projections: 3 * (L * d_model * d_model)\n",
    "    flops_qkv = 3 * L * d_model * d_model\n",
    "    d_head = d_model // n_heads\n",
    "    # attention scores & weighted sum: per head: L*L*d_head *2 (matmul multiply-add)\n",
    "    flops_scores = n_heads * (2 * (L*L*d_head))\n",
    "    flops_weighted = n_heads * (2 * (L*L*d_head))  # P@V\n",
    "    # output projection Wo: L * d_model * d_model\n",
    "    flops_wo = L * d_model * d_model\n",
    "    # MLP: two linears: L * d_model * (d_model*mlp_ratio) *2\n",
    "    flops_mlp = 2 * L * d_model * (d_model*mlp_ratio)\n",
    "    total = flops_qkv + flops_scores + flops_weighted + flops_wo + flops_mlp\n",
    "    return total\n",
    "\n",
    "@torch.no_grad()\n",
    "def time_block(B=16, L=256, d_model=256, n_heads_list=(1,2,4,8), iters=20, warmup=10):\n",
    "    times = {}\n",
    "    x = torch.randn(B, L, d_model, device=device)\n",
    "    for h in n_heads_list:\n",
    "        blk = TransformerBlock(d_model=d_model, n_heads=h, dropout=0.0, causal=False).to(device)\n",
    "        # warmup\n",
    "        for _ in range(warmup):\n",
    "            _ = blk(x)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        for _ in range(iters):\n",
    "            _ = blk(x)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        times[h] = (t1 - t0) / iters\n",
    "    return times\n",
    "\n",
    "# Example numbers\n",
    "L, D = 256, 256\n",
    "for h in [1,2,4,8]:\n",
    "    print(f\"Heads={h}: est FLOPs/block ~ {estimate_block_flops(L, D, h)/1e9:.2f} GFLOPs\")\n",
    "print(\"Timing (s/iter):\", time_block(B=8, L=L, d_model=D, n_heads_list=(1,2,4,8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff12853",
   "metadata": {},
   "source": [
    "\n",
    "### **III.** Optional tasks: train on **Addition** and **Parity**\n",
    "\n",
    "Swap the dataset and the model; observe that:\n",
    "- Naive CNN is weaker (especially on long dependencies),  \n",
    "- Dilated CNN closes much of the gap,  \n",
    "- Transformer remains strong with the right depth/heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f292f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parity (binary, 0/1)\n",
    "cfg = TrainConfig(epochs=3, batch_size=128, lr=2e-3, max_batches_per_epoch=150)\n",
    "train_ds = ParityDataset(n=8000, L=128)\n",
    "val_ds   = ParityDataset(n=2000, L=256)  # harder generalization\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=bin_pad_collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=bin_pad_collate)\n",
    "\n",
    "print(\"Naive CNN on Parity\")\n",
    "model = NaiveCNN(vocab_size=2, emb_dim=16, hidden=64, n_layers=2, kernel=3, num_classes=2)\n",
    "model, acc_naive = train_eval(model, train_loader, val_loader, cfg)\n",
    "\n",
    "print(\"\\nDilated CNN on Parity\")\n",
    "model = DilatedCNN(vocab_size=2, emb_dim=32, hidden=96, layers_cfg=((3,1),(3,2),(3,4),(3,8)), num_classes=2)\n",
    "model, acc_dil = train_eval(model, train_loader, val_loader, cfg)\n",
    "\n",
    "print(\"\\nTransformer on Parity\")\n",
    "model = TinyTransformer(vocab_size=2, d_model=64, n_layers=2, n_heads=4, num_classes=2)\n",
    "model, acc_tx = train_eval(model, train_loader, val_loader, cfg)\n",
    "\n",
    "print(f\"Val acc: naive {acc_naive:.3f} | dilated {acc_dil:.3f} | transformer {acc_tx:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543da23",
   "metadata": {},
   "source": [
    "\n",
    "### **Appendix A.** Simple training tips\n",
    "- Use `AdamW`, small number of epochs for classroom speed.\n",
    "- Clip gradients to stabilize early training.\n",
    "- Monitor validation accuracy; keep best checkpoint in RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc65fb",
   "metadata": {},
   "source": [
    "\n",
    "### **Appendix B.** GPU in Colab\n",
    "Ensure **GPU** is enabled: *Runtime → Change runtime type → Hardware accelerator: GPU*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
