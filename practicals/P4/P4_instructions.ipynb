{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3db6482b",
      "metadata": {
        "id": "3db6482b"
      },
      "source": [
        "\n",
        "# Practical session n°4 :\n",
        "\n",
        "Notions:\n",
        "\n",
        "\n",
        "*   Attentional layers\n",
        "*   Multiple attentional heads\n",
        "*   Standard toy tasks (Dyck validaty test, addition, parity test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc8d855",
      "metadata": {
        "id": "2bc8d855"
      },
      "source": [
        "## Part I — Presentation of the Transformer architecture\n",
        "\n",
        "In this part, you will:\n",
        "\n",
        "1. See how a tiny Transformer is coded in PyTorch.\n",
        "2. Focus on the attention layer: multi-head mechanism and the role of the Q, K, V matrices.\n",
        "3. Estimate how many FLOPs are involved in a forward pass through a multi-head attention layer  \n",
        "   (and compare with the theoretical considerations from the lesson).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & basic configuration\n",
        "\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_printoptions(precision=4, sci_mode=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZagGANXs-hI",
        "outputId": "73ad725d-08ec-4eab-afa4-4b8b9ef9198d"
      },
      "id": "fZagGANXs-hI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiny Transformer building blocks (embedding + multi-head attention)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 1024):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, D)\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:, :L, :]\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal multi-head self-attention.\n",
        "    If verbose=True, prints tensor shapes at key steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int = 64, n_heads: int = 4, verbose: bool = False):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def _vprint(self, name, x):\n",
        "        if self.verbose:\n",
        "            print(f\"{name}: {tuple(x.shape)}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, D)\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        Q = self.Wq(x)  # (B, L, D)\n",
        "        K = self.Wk(x)\n",
        "        V = self.Wv(x)\n",
        "        self._vprint(\"Q\", Q)\n",
        "        self._vprint(\"K\", K)\n",
        "        self._vprint(\"V\", V)\n",
        "\n",
        "        # reshape to heads\n",
        "        Q = Q.view(B, L, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, L, Dh)\n",
        "        K = K.view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        self._vprint(\"Q_heads\", Q)\n",
        "        self._vprint(\"K_heads\", K)\n",
        "        self._vprint(\"V_heads\", V)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B,H,L,L)\n",
        "        self._vprint(\"scores\", scores)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)  # (B,H,L,L)\n",
        "        self._vprint(\"attn_probs\", attn)\n",
        "\n",
        "        Z = torch.matmul(attn, V)  # (B,H,L,Dh)\n",
        "        self._vprint(\"Z_heads\", Z)\n",
        "\n",
        "        # merge heads\n",
        "        Z = Z.transpose(1, 2).contiguous().view(B, L, D)  # (B, L, D)\n",
        "        self._vprint(\"Z_merged\", Z)\n",
        "\n",
        "        out = self.Wo(Z)  # (B, L, D)\n",
        "        self._vprint(\"out\", out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int = 64, n_heads: int = 4, mlp_ratio: int = 4):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = MultiHeadSelfAttention(d_model=d_model, n_heads=n_heads, verbose=False)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * mlp_ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model * mlp_ratio, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Tiny Transformer for sequence classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int = 64, n_heads: int = 4, n_layers: int = 2, num_classes: int = 2):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(d_model=d_model, n_heads=n_heads) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L) token ids\n",
        "        h = self.tok_emb(x)\n",
        "        h = self.pos_emb(h)\n",
        "        for blk in self.blocks:\n",
        "            h = blk(h)\n",
        "        # use mean pooling for classification\n",
        "        h = h.mean(dim=1)\n",
        "        return self.head(h)\n"
      ],
      "metadata": {
        "id": "N_ikSjeMtBUt"
      },
      "id": "N_ikSjeMtBUt",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 1 — Check embedding output size\n",
        "\n",
        "Using `TinyTransformer`, define a random input batch of token ids with:\n",
        "\n",
        "- batch size `B`\n",
        "- sequence length `L`\n",
        "- vocabulary size consistent with the model\n",
        "\n",
        "Use `torch.randint` to generate this input and pass it **only through the embedding layer**.  \n",
        "Verify that the output has shape `(B, L, d_model)`."
      ],
      "metadata": {
        "id": "TgJt9nGgtUcs"
      },
      "id": "TgJt9nGgtUcs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correction Exercice 1\n",
        "\n",
        "B, L = 4, 10\n",
        "vocab_size = 20\n",
        "d_model = 64\n",
        "\n",
        "model = TinyTransformer(vocab_size=vocab_size, d_model=d_model, n_heads=4, n_layers=2, num_classes=2)\n",
        "\n",
        "x = torch.randint(low=0, high=vocab_size, size=(B, L))  # (B, L)\n",
        "with torch.no_grad():\n",
        "    emb_out = model.tok_emb(x)  # (B, L, d_model)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Embedding output shape:\", emb_out.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "mhgeyjsitI6p"
      },
      "id": "mhgeyjsitI6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 2 — Pass through MultiHeadSelfAttention (with verbose shapes)\n",
        "\n",
        "1. Reuse the embedded tensor from Exercice 1 (or recreate it).\n",
        "2. Instantiate a `MultiHeadSelfAttention` layer with `verbose=True`.\n",
        "3. Pass the embeddings through it and observe the printed shapes at each step.\n",
        "\n",
        "This is to **trace Q, K, V, heads, scores, and outputs**.\n"
      ],
      "metadata": {
        "id": "cPcdxu-CtgEh"
      },
      "id": "cPcdxu-CtgEh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correction Exercice 2\n",
        "\n",
        "B, L = 2, 6\n",
        "vocab_size = 20\n",
        "d_model = 64\n",
        "n_heads = 4\n",
        "\n",
        "x = torch.randint(0, vocab_size, (B, L))\n",
        "emb = model.tok_emb(x)\n",
        "\n",
        "attn = MultiHeadSelfAttention(d_model=d_model, n_heads=n_heads, verbose=True)\n",
        "with torch.no_grad():\n",
        "    out = attn(emb)\n",
        "\n",
        "print(\"Final attention output shape:\", out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGzvX2I0tSVM",
        "outputId": "02bdb2d9-2947-4a25-df1f-e2ad8e295abd"
      },
      "id": "lGzvX2I0tSVM",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: (2, 6, 64)\n",
            "K: (2, 6, 64)\n",
            "V: (2, 6, 64)\n",
            "Q_heads: (2, 4, 6, 16)\n",
            "K_heads: (2, 4, 6, 16)\n",
            "V_heads: (2, 4, 6, 16)\n",
            "scores: (2, 4, 6, 6)\n",
            "attn_probs: (2, 4, 6, 6)\n",
            "Z_heads: (2, 4, 6, 16)\n",
            "Z_merged: (2, 6, 64)\n",
            "out: (2, 6, 64)\n",
            "Final attention output shape: torch.Size([2, 6, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 3 — Count FLOPs in multi-head self-attention\n",
        "\n",
        "For a single multi-head self-attention layer with:\n",
        "\n",
        "- sequence length `L`\n",
        "- model dimension `d_model`\n",
        "- number of heads `h` (each of size `d_head = d_model / h`)\n",
        "\n",
        "1. Derive the approximate number of multiply-add operations for:\n",
        "   - Q, K, V linear projections\n",
        "   - Attention scores computation\n",
        "   - Attention-weighted sum\n",
        "   - Output projection\n",
        "2. Identify the term that dominates when `L` is large, and explain why it is in `O(L² × d_model)`.\n",
        "\n",
        "Complete the function below.\n"
      ],
      "metadata": {
        "id": "7K_D-zOFtvlx"
      },
      "id": "7K_D-zOFtvlx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correction Exercice 3 & 4 — FLOPs estimates\n",
        "\n",
        "def mhsa_flops(L, d_model, n_heads):\n",
        "    \"\"\"\n",
        "    Rough FLOPs (multiply-adds counted as ~2 ops, but we keep a simple count).\n",
        "    Returns a dict with contributions.\n",
        "    \"\"\"\n",
        "    d_head = d_model // n_heads\n",
        "\n",
        "    # Q, K, V projections: 3 * (L * d_model * d_model)\n",
        "    qkv = 3 * L * d_model * d_model\n",
        "\n",
        "    # scores = Q K^T: (B,H,L,Dh) x (B,H,Dh,L) -> (B,H,L,L)\n",
        "    # ~ 2 * (H * L * L * Dh)\n",
        "    scores = 2 * n_heads * (L * L * d_head)\n",
        "\n",
        "    # attn @ V: (B,H,L,L) x (B,H,L,Dh) -> (B,H,L,Dh)\n",
        "    # ~ 2 * (H * L * L * Dh)\n",
        "    weighted = 2 * n_heads * (L * L * d_head)\n",
        "\n",
        "    # output projection Wo: L * d_model * d_model\n",
        "    wo = L * d_model * d_model\n",
        "\n",
        "    return {\n",
        "        \"qkv\": qkv,\n",
        "        \"scores\": scores,\n",
        "        \"weighted\": weighted,\n",
        "        \"wo\": wo,\n",
        "        \"total\": qkv + scores + weighted + wo,\n",
        "    }\n",
        "\n",
        "# Example: impact of L (show domination of L^2 * d_model terms)\n",
        "for L in [16, 64, 256]:\n",
        "    stats = mhsa_flops(L=L, d_model=64, n_heads=4)\n",
        "    print(f\"L={L} -> total={stats['total']:.2e}, scores+weighted={stats['scores']+stats['weighted']:.2e}\")\n",
        "\n",
        "print(\"\\nObservation: for large L, the attention matrix computations (in L^2) dominate.\")\n",
        "\n",
        "# Exercice 4 — Example: FLOPs for a tiny Transformer on a batch\n",
        "def tiny_transformer_flops(num_layers, L, d_model, n_heads, mlp_ratio=4):\n",
        "    attn = mhsa_flops(L, d_model, n_heads)[\"total\"]\n",
        "    # MLP per layer: 2 linear layers: ~ 2 * L * d_model * (d_model * mlp_ratio)\n",
        "    mlp = 2 * L * d_model * (d_model * mlp_ratio)\n",
        "    per_layer = attn + mlp\n",
        "    return num_layers * per_layer\n",
        "\n",
        "print(\"\\nTinyTransformer example (num_layers=2, L=64, d_model=64, n_heads=4):\")\n",
        "print(f\"~ {tiny_transformer_flops(2, 64, 64, 4):.2e} FLOPs (rough order of magnitude)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQL1gdEPt3rH",
        "outputId": "9d23e9d5-fe5c-4aed-f904-c65e8211593a"
      },
      "id": "TQL1gdEPt3rH",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L=16 -> total=3.28e+05, scores+weighted=6.55e+04\n",
            "L=64 -> total=2.10e+06, scores+weighted=1.05e+06\n",
            "L=256 -> total=2.10e+07, scores+weighted=1.68e+07\n",
            "\n",
            "Observation: for large L, the attention matrix computations (in L^2) dominate.\n",
            "\n",
            "TinyTransformer example (num_layers=2, L=64, d_model=64, n_heads=4):\n",
            "~ 8.39e+06 FLOPs (rough order of magnitude)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II — A first example: Validation of Dyck-1 words\n",
        "\n",
        "We now train models to decide if a word over `{ '(', ')' }` is a valid Dyck-1 word (balanced parentheses).\n",
        "\n",
        "We start with a simple dataset **without distractors**.\n"
      ],
      "metadata": {
        "id": "v335uURwuFUX"
      },
      "id": "v335uURwuFUX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dyck-1 dataset (no distractors)\n",
        "\n",
        "VOCAB_DYCK = ['(', ')']\n",
        "stoi_dyck = {c: i for i, c in enumerate(VOCAB_DYCK)}\n",
        "itos_dyck = {i: c for c, i in stoi_dyck.items()}\n",
        "\n",
        "def dyck1_valid_sequence(n_pairs):\n",
        "    \"\"\"\n",
        "    Generate a uniformly random valid Dyck-1 sequence with n_pairs pairs.\n",
        "    \"\"\"\n",
        "    seq = []\n",
        "    depth = 0\n",
        "    for _ in range(2 * n_pairs):\n",
        "        if depth == 0:\n",
        "            seq.append('(')\n",
        "            depth += 1\n",
        "        elif depth == (2 * n_pairs - len(seq)):\n",
        "            seq.append(')')\n",
        "            depth -= 1\n",
        "        else:\n",
        "            if random.random() < 0.5:\n",
        "                seq.append('(')\n",
        "                depth += 1\n",
        "            else:\n",
        "                seq.append(')')\n",
        "                depth -= 1\n",
        "    return ''.join(seq)\n",
        "\n",
        "def is_valid_dyck1(s):\n",
        "    depth = 0\n",
        "    for ch in s:\n",
        "        if ch == '(':\n",
        "            depth += 1\n",
        "        elif ch == ')':\n",
        "            depth -= 1\n",
        "        if depth < 0:\n",
        "            return False\n",
        "    return depth == 0\n",
        "\n",
        "class DyckDataset(nn.Module):\n",
        "    \"\"\"\n",
        "    Binary classification:\n",
        "    y = 1 if valid Dyck-1, y = 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples=2000, n_pairs=6):\n",
        "        super().__init__()\n",
        "        self.samples = []\n",
        "        for _ in range(n_samples):\n",
        "            s = dyck1_valid_sequence(n_pairs)\n",
        "            # with prob 0.5 corrupt one char to create invalid string\n",
        "            if random.random() < 0.5:\n",
        "                y = 1\n",
        "            else:\n",
        "                s = list(s)\n",
        "                pos = random.randrange(len(s))\n",
        "                s[pos] = '(' if s[pos] == ')' else ')'\n",
        "                s = ''.join(s)\n",
        "                y = int(is_valid_dyck1(s))\n",
        "                # if by chance it is valid, flip again\n",
        "                if y == 1:\n",
        "                    pos = (pos + 1) % len(s)\n",
        "                    s = list(s)\n",
        "                    s[pos] = '(' if s[pos] == ')' else ')'\n",
        "                    s = ''.join(s)\n",
        "                    y = int(is_valid_dyck1(s))\n",
        "            x = torch.tensor([stoi_dyck[c] for c in s], dtype=torch.long)\n",
        "            self.samples.append((x, torch.tensor(y, dtype=torch.long)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "def dyck_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    L = max(len(x) for x in xs)\n",
        "    X = torch.full((len(xs), L), fill_value=0, dtype=torch.long)\n",
        "    for i, x in enumerate(xs):\n",
        "        X[i, :len(x)] = x\n",
        "    Y = torch.stack(ys)\n",
        "    return X, Y\n"
      ],
      "metadata": {
        "id": "TN2FJTOGuLYY"
      },
      "id": "TN2FJTOGuLYY",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 5 — Inspect Dyck-1 samples\n",
        "\n",
        "1. Build a `DyckDataset` with `n_pairs = 6`.\n",
        "2. Print the first 5 sequences:\n",
        "   - as indices\n",
        "   - as corresponding parenthesis strings\n",
        "   - with their labels (valid / invalid)\n",
        "3. What is the size of the vocabulary for this problem?\n"
      ],
      "metadata": {
        "id": "qGyMFcCeunqR"
      },
      "id": "qGyMFcCeunqR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correction Exercice 5\n",
        "\n",
        "dyck_train = DyckDataset(n_samples=8, n_pairs=6)\n",
        "\n",
        "for i in range(5):\n",
        "    x, y = dyck_train[i]\n",
        "    s = ''.join(itos_dyck[int(t)] for t in x)\n",
        "    print(f\"Sample {i}: indices={x.tolist()}  | word={s}  | label={int(y)}\")\n",
        "\n",
        "print(\"\\nVocabulary:\", VOCAB_DYCK)\n",
        "print(\"Vocab size:\", len(VOCAB_DYCK))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEkcsYYKupbc",
        "outputId": "972fe7b9-6bc1-4a53-eb60-46e6501485a3"
      },
      "id": "OEkcsYYKupbc",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0: indices=[0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1]  | word=()())(()()))  | label=0\n",
            "Sample 1: indices=[0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1]  | word=()(()((())))  | label=1\n",
            "Sample 2: indices=[0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  | word=()()()()()()  | label=1\n",
            "Sample 3: indices=[0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1]  | word=((((()(())))  | label=0\n",
            "Sample 4: indices=[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1]  | word=()(()(()(())  | label=0\n",
            "\n",
            "Vocabulary: ['(', ')']\n",
            "Vocab size: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 6 — Training loop with history + learning curves\n",
        "\n",
        "Complete a training loop that:\n",
        "\n",
        "1. Trains a small Transformer on Dyck-1.\n",
        "2. Stores training and validation accuracies at each epoch.\n",
        "3. Plots both curves on the same figure.\n",
        "\n",
        "Use the `TinyTransformer` from Part I (with `vocab_size = 2`).\n"
      ],
      "metadata": {
        "id": "hBv9HwK0uwXl"
      },
      "id": "hBv9HwK0uwXl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training utilities (Transformer on Dyck-1) with history\n",
        "\n",
        "def accuracy_from_logits(logits, y):\n",
        "    return (logits.argmax(dim=-1) == y).float().mean().item()\n",
        "\n",
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for X, Y in loader:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = F.cross_entropy(logits, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        bs = X.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy_from_logits(logits, Y) * bs\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    for X, Y in loader:\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        logits = model(X)\n",
        "        loss = F.cross_entropy(logits, Y)\n",
        "        bs = X.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy_from_logits(logits, Y) * bs\n",
        "        n += bs\n",
        "    return total_loss / n, total_acc / n\n"
      ],
      "metadata": {
        "id": "U76FkXdku1XZ"
      },
      "id": "U76FkXdku1XZ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Transformer on Dyck-1 and plot curves\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = DyckDataset(n_samples=2000, n_pairs=6)\n",
        "val_set   = DyckDataset(n_samples=500,  n_pairs=6)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=64, shuffle=False)\n",
        "\n",
        "model_tx = TinyTransformer(vocab_size=len(VOCAB_DYCK), d_model=64, n_heads=4, n_layers=2, num_classes=2).to(device)\n",
        "opt = torch.optim.AdamW(model_tx.parameters(), lr=2e-3)\n",
        "\n",
        "E = 8\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "for epoch in range(1, E+1):\n",
        "    tr_loss, tr_acc = train_epoch(model_tx, train_loader, opt)\n",
        "    va_loss, va_acc = eval_epoch(model_tx, val_loader)\n",
        "    train_accs.append(tr_acc)\n",
        "    val_accs.append(va_acc)\n",
        "    print(f\"Epoch {epoch}: train_acc={tr_acc:.3f}, val_acc={va_acc:.3f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, E+1), train_accs, label=\"train\")\n",
        "plt.plot(range(1, E+1), val_accs, label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Dyck-1 — TinyTransformer\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hct-feEqu5fN"
      },
      "id": "Hct-feEqu5fN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 7 — Compare with a CNN baseline\n",
        "\n",
        "1. Implement a 1D CNN below.\n",
        "2. Train it on the same Dyck-1 dataset.\n",
        "3. Compare its validation accuracy and behavior with the Transformer.\n",
        "\n",
        "Use the correction cell below only after you have tried.\n"
      ],
      "metadata": {
        "id": "hnDzwsjxvIpI"
      },
      "id": "hnDzwsjxvIpI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correction Exercice 7 — Simple CNN baseline on Dyck-1\n",
        "\n",
        "class DyckCNN(nn.Module):\n",
        "    def __init__(self, vocab_size=2, emb_dim=32, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.conv1 = nn.Conv1d(emb_dim, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "        self.head = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L)\n",
        "        e = self.emb(x).transpose(1, 2)  # (B,C,L)\n",
        "        h = F.relu(self.conv1(e))\n",
        "        h = F.relu(self.conv2(h))\n",
        "        h = h.mean(dim=-1)               # global average pooling\n",
        "        return self.head(h)\n",
        "\n",
        "cnn = DyckCNN(vocab_size=len(VOCAB_DYCK), emb_dim=32, num_classes=2).to(device)\n",
        "opt_cnn = torch.optim.AdamW(cnn.parameters(), lr=2e-3)\n",
        "\n",
        "E = 8\n",
        "for epoch in range(1, E+1):\n",
        "    tr_loss, tr_acc = train_epoch(cnn, train_loader, opt_cnn)\n",
        "    va_loss, va_acc = eval_epoch(cnn, val_loader)\n",
        "    print(f\"[CNN] Epoch {epoch}: train_acc={tr_acc:.3f}, val_acc={va_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "Y-aVL-3VvIC9"
      },
      "id": "Y-aVL-3VvIC9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part III — Addition and Parity tasks\n",
        "\n",
        "### Exercice 11\n",
        "\n",
        "Use the following synthetic datasets to train and compare models\n",
        "(Transformers and CNNs) on:\n",
        "\n",
        "1. **Addition**: predict the last digit of `a + b` from an input like `\"123+45=\"`.\n",
        "2. **Parity**: predict the parity (0/1) of a binary string.\n",
        "\n",
        "Reuse your training utilities from Part II.\n"
      ],
      "metadata": {
        "id": "dL_O3f55vTXv"
      },
      "id": "dL_O3f55vTXv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Addition & Parity datasets\n",
        "\n",
        "# --- Addition ---\n",
        "DIGITS = [str(i) for i in range(10)]\n",
        "ADD_VOCAB = DIGITS + ['+', '=']\n",
        "add_stoi = {c: i for i, c in enumerate(ADD_VOCAB)}\n",
        "\n",
        "def gen_add_sample(n1_digits=3, n2_digits=3):\n",
        "    \"\"\"\n",
        "    Input:  string representation \"a+b=\".\n",
        "    Target: last digit of (a + b).\n",
        "    \"\"\"\n",
        "    a = random.randint(0, 10**n1_digits - 1)\n",
        "    b = random.randint(0, 10**n2_digits - 1)\n",
        "    s = f\"{a}+{b}=\"\n",
        "    x = torch.tensor([add_stoi[c] for c in s], dtype=torch.long)\n",
        "    y = (a + b) % 10\n",
        "    return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class AddDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, n, n1_digits=3, n2_digits=3):\n",
        "        self.samples = [gen_add_sample(n1_digits, n2_digits) for _ in range(n)]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i): return self.samples[i]\n",
        "\n",
        "def add_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    L = max(len(x) for x in xs)\n",
        "    X = torch.full((len(xs), L), fill_value=0, dtype=torch.long)\n",
        "    for i, x in enumerate(xs):\n",
        "        X[i, :len(x)] = x\n",
        "    Y = torch.stack(ys)\n",
        "    return X, Y\n",
        "\n",
        "# --- Parity ---\n",
        "BIN_VOCAB = ['0', '1']\n",
        "bin_stoi = {c: i for i, c in enumerate(BIN_VOCAB)}\n",
        "\n",
        "def gen_parity_sample(L=32):\n",
        "    \"\"\"\n",
        "    Input:  random binary string of length L.\n",
        "    Target: parity of number of '1's (0 = even, 1 = odd).\n",
        "    \"\"\"\n",
        "    s = ''.join(random.choice(BIN_VOCAB) for _ in range(L))\n",
        "    x = torch.tensor([bin_stoi[c] for c in s], dtype=torch.long)\n",
        "    y = s.count('1') % 2\n",
        "    return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "class ParityDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, n, L=32):\n",
        "        self.samples = [gen_parity_sample(L) for _ in range(n)]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i): return self.samples[i]\n",
        "\n",
        "def parity_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    L = max(len(x) for x in xs)\n",
        "    X = torch.full((len(xs), L), fill_value=0, dtype=torch.long)\n",
        "    for i, x in enumerate(xs):\n",
        "        X[i, :len(x)] = x\n",
        "    Y = torch.stack(ys)\n",
        "    return X, Y\n"
      ],
      "metadata": {
        "id": "HjFRFST-vWzS"
      },
      "id": "HjFRFST-vWzS",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reusable training helper (supports different num_classes)\n",
        "\n",
        "def train_model_classifier(model, train_loader, val_loader, epochs=6, lr=2e-3, name=\"model\"):\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val = 0.0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, opt)\n",
        "        va_loss, va_acc = eval_epoch(model, val_loader)\n",
        "        if va_acc > best_val:\n",
        "            best_val = va_acc\n",
        "        print(f\"[{name}] Epoch {ep}: train_acc={tr_acc:.3f}, val_acc={va_acc:.3f}\")\n",
        "    print(f\"[{name}] best_val_acc={best_val:.3f}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "l77JtPjXvcbK"
      },
      "id": "l77JtPjXvcbK",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: TinyTransformer on Addition & Parity\n",
        "\n",
        "# Addition\n",
        "train_add = AddDataset(n=4000)\n",
        "val_add   = AddDataset(n=1000)\n",
        "\n",
        "train_add_loader = torch.utils.data.DataLoader(train_add, batch_size=64, shuffle=True,  collate_fn=add_collate)\n",
        "val_add_loader   = torch.utils.data.DataLoader(val_add,   batch_size=64, shuffle=False, collate_fn=add_collate)\n",
        "\n",
        "tx_add = TinyTransformer(vocab_size=len(ADD_VOCAB), d_model=64, n_heads=4, n_layers=2, num_classes=10)\n",
        "train_model_classifier(tx_add, train_add_loader, val_add_loader, epochs=6, name=\"Transformer-Add\")\n",
        "\n",
        "# Parity\n",
        "train_par = ParityDataset(n=4000, L=64)\n",
        "val_par   = ParityDataset(n=1000, L=64)\n",
        "\n",
        "train_par_loader = torch.utils.data.DataLoader(train_par, batch_size=64, shuffle=True,  collate_fn=parity_collate)\n",
        "val_par_loader   = torch.utils.data.DataLoader(val_par,   batch_size=64, shuffle=False, collate_fn=parity_collate)\n",
        "\n",
        "tx_par = TinyTransformer(vocab_size=len(BIN_VOCAB), d_model=64, n_heads=4, n_layers=2, num_classes=2)\n",
        "train_model_classifier(tx_par, train_par_loader, val_par_loader, epochs=6, name=\"Transformer-Parity\")\n"
      ],
      "metadata": {
        "id": "dVUFh8pxvgkd"
      },
      "id": "dVUFh8pxvgkd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}